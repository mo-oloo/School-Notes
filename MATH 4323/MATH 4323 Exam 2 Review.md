##### Supervised Learning
- Methods: K-Nearest Neighbors (KNN) and Support Vector Machines (SVM)
- Goal is to predict/explain the response using the predictors
- $p$ features with features $X=X_1, X_2, ..., X_p$ measured on $n$ observations
- *Examples*: Given the stock price changes in the previous 5 days, predict the price change today.

##### Unsupervised Learning
- ***No response variable $Y$ ***
- $p$ features ${X = X_1, X_2, ..., X_p}$ measures on $n$ observations
- **Goal:** We focus on discovering patterns & relationships among measurements $X_1, .... X_p$ and among the $n$ observations.
	- *Can we discover groupings among the variables or among the observations?*
- *Examples*: Given demographic and spending information on customers, proceed to *group* them by similarity

- Unsupervised Learning is much more subjective; There is no simple goal for the data analysis such as predicting a response.
	- It is often performed as part of an exploratory data analysis.
	- No easy way to assess the quality of the solution; no universally accepted method of cross-validation or validating the results on a data set

##### Unsupervised Learning Techniques
- **Principle Component Analysis (PCA)**: Tool for grouping the predictor variables $X = X_1,...,X_p$ that's used for data visualization or data pre-processing before supervised techniques are applied
- **Clustering**: Methods for grouping the observations allowing for discovery of unknown subgroups in the data.

### Principle Component Analysis
###### Motivation for PCA
- How would we visualize $n$ observatiosn on $p$ features?
- One approach is creating 2D plots of the data comparing 2 features.
	- Problem: This creates $p \choose 2$ plots, which is unfeasible if $p$ is large.
	- Problem: Most plots will not be informative since they contain just a small fraction of the total information present in the dataset. Comparing the covariance will be impossible to study and interprety with large $p$. 

##### PCA
- PCA gives us a low-dimensional representation of the dataset.
- It finds a sequence of linear combinations of the variables that contains as much of the variation as possible
	- Each linear combination corresponds to a principle component.
	- If we can obtain a low (two) dimensional representation of the data that captures a lot of the information, then we can plot the observations in low-dimensional space

##### How to find principle components
- The first principle component is the normalized linear combination of the features $$Z_1 = \phi_{11}X_1+\phi_{21}X_2+...+\phi_{p1}X_p$$ that has the largest variance, where $\sum_{j=1}^p \phi_{j1}^2=1$
	- $\phi_{11},...,\phi_{p1}$ are the **loadings** of the **first** principle components. The loadings make up the PC loading vector, $\phi_1 = (\phi_{11},...,\phi_{p1})^T$.
	- We constrain the loadings so their SS equals 1.

- Suppose we have an $n$ X $p$ data set **X**. Assume that each column in **X** has a mean of 0 (each variable has a mean of 0).
- We then look for the linear combination of the sample feature values of the form $$Z_{i1} = \phi_{11}x_{i1} + \phi_{21}x_{i2} + ... + \phi_{p1}x_{ip}$$ for $i = 1,...,n$ that has largest sample variance, subject to the constraint of $$\sum_{j=1}^p \phi_{j1}^2 = 1$$
- Since each of the $x_{ij}$ has a mean of zero, then that means so does $Z_{i1}$.
- The variance of $Z_{i1}$ can be written as $$\frac{1}{n}\sum_{i=1}^nZ_{i1}^2$$
- The first principal component loading vector is an optimization problem: $$\underset{\phi_{11},...,\phi_{p1}}{max} \Biggl\{\frac{1}{n}\sum_{i=1}^n \Bigg(\sum_{j=1}^n\phi_{j1}x_{ij}\Bigg)^2\Bigg\} \ subject \ to \ \sum_{j=1}^p\phi_{j1}^2=1$$

